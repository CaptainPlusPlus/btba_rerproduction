{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "132s9GYcx6ab5EY4Y8ZA_e3aBdwtdOQ6N",
      "authorship_tag": "ABX9TyOVxTktZSXSKC0QvJnOwj5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed003cb64d0d4c12b8edb18584ad0e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e46c8612c73b441d80e98fae13f8eeeb",
              "IPY_MODEL_295f3956e5f346cfaf526319e0da3c6a",
              "IPY_MODEL_da7805f3950c40d7805bcd0c9b6b34bf"
            ],
            "layout": "IPY_MODEL_18ff8a5dc5d541348e75d85cfd3b3451"
          }
        },
        "e46c8612c73b441d80e98fae13f8eeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10d679ba111347c392fb702b54936930",
            "placeholder": "​",
            "style": "IPY_MODEL_ebb1b56af56246839a91f6acd95bea5c",
            "value": "config.json: 100%"
          }
        },
        "295f3956e5f346cfaf526319e0da3c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1849ee8975f34731bce298e833fa9016",
            "max": 1628,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e88b101a7f1c4e688d1ea31244fc0c0d",
            "value": 1628
          }
        },
        "da7805f3950c40d7805bcd0c9b6b34bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0a6b3f3c01b4883af38e58070ace523",
            "placeholder": "​",
            "style": "IPY_MODEL_e64d43ed390e4bdb844de23c0076a2f3",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 132kB/s]"
          }
        },
        "18ff8a5dc5d541348e75d85cfd3b3451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d679ba111347c392fb702b54936930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebb1b56af56246839a91f6acd95bea5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1849ee8975f34731bce298e833fa9016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88b101a7f1c4e688d1ea31244fc0c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0a6b3f3c01b4883af38e58070ace523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e64d43ed390e4bdb844de23c0076a2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaptainPlusPlus/btba_rerproduction/blob/main/btba_reproduction2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproduction of BTBA Model for Unsupervised Word Alignment\n",
        "Article can be found here: https://aclanthology.org/2021.acl-long.24.pdf"
      ],
      "metadata": {
        "id": "ExMtdui5y-3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "* Downloand and preprocess the deen, fren, roen texts from https://github.com/lilt/alignment-scripts.\n",
        "* Upload the `bpe` lowercased preprocessed `train` & `test` folders as well as the sentencepiece `bpe` models."
      ],
      "metadata": {
        "id": "SmCCmEo3yXov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Tokenizer Definition\n",
        "\n",
        "Since the Sentencepiece tokenizer used in the article and alignment scripts it is compared against outputs a binary format model and vocabulary, the sentencepiece tokenizer must be adjusted to fit the HuggingFace models used to reproduce the article's transformer based approaach."
      ],
      "metadata": {
        "id": "Ap-m1E74yOte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0_1oRQYyyGXl"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "class CustomSentencePieceTokenizer:\n",
        "    def __init__(self, sentencepiece_model_path):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        if not self.sp.Load(sentencepiece_model_path):\n",
        "            raise FileNotFoundError(\"Failed to load SentencePiece model from specified path.\")\n",
        "        self.special_tokens = {'<s>': self.sp.piece_to_id('<s>'), '</s>': self.sp.piece_to_id('</s>'), '<unk>': self.sp.piece_to_id('<unk>')}\n",
        "        self.additional_special_tokens = {'<pad>': self.sp.GetPieceSize(), '<mask>': self.sp.GetPieceSize() + 1}\n",
        "        self.special_token_ids = {**self.special_tokens, **self.additional_special_tokens}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self.sp.encode_as_pieces(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self.special_token_ids.get(token, self.sp.piece_to_id(token)) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        id_to_token_map = {id: token for token, id in self.special_token_ids.items()}\n",
        "        id_to_token_map.update({id: self.sp.id_to_piece(id) for id in range(self.sp.GetPieceSize())})\n",
        "        return [id_to_token_map.get(id, '<unk>') for id in ids]\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.sp.GetPieceSize() + len(self.additional_special_tokens)\n",
        "\n",
        "    def get_special_tokens(self):\n",
        "        return {**self.special_tokens, **self.additional_special_tokens}\n",
        "\n",
        "    def get_special_token_ids(self):\n",
        "        return self.special_token_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_custom_tokenizer():\n",
        "    tokenizer = CustomSentencePieceTokenizer('/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/bpe.deen.model')\n",
        "    test_sentence = \"das ist ein test.\"\n",
        "    print(\"Testing tokenization of sentence:\", test_sentence)\n",
        "    tokens = tokenizer.tokenize(test_sentence)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    print(\"Token IDs:\", token_ids)\n",
        "    tokens_from_ids = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    print(\"Tokens from IDs:\", tokens_from_ids)\n",
        "    print(\"Special Tokens:\", tokenizer.get_special_tokens())\n",
        "    print(\"Special Token IDs:\", tokenizer.get_special_token_ids())\n",
        "    special_tokens_test = ['<pad>', '<mask>', '<s>', '</s>', '<unk>']\n",
        "    special_tokens_ids = tokenizer.convert_tokens_to_ids(special_tokens_test)\n",
        "    print(\"Special tokens to IDs:\", list(zip(special_tokens_test, special_tokens_ids)))\n",
        "    special_tokens_round_trip = tokenizer.convert_ids_to_tokens(special_tokens_ids)\n",
        "    print(\"IDs back to special tokens:\", special_tokens_round_trip)\n",
        "\n",
        "test_custom_tokenizer()"
      ],
      "metadata": {
        "id": "vu6WLpwz04Jc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d1d713-1576-496c-b4d9-9997b22ecf55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing tokenization of sentence: das ist ein test.\n",
            "Tokens: ['▁das', '▁ist', '▁ein', '▁test', '.']\n",
            "Token IDs: [94, 158, 69, 4218, 39789]\n",
            "Tokens from IDs: ['▁das', '▁ist', '▁ein', '▁test', '.']\n",
            "Special Tokens: {'<s>': 1, '</s>': 2, '<unk>': 0, '<pad>': 40000, '<mask>': 40001}\n",
            "Special Token IDs: {'<s>': 1, '</s>': 2, '<unk>': 0, '<pad>': 40000, '<mask>': 40001}\n",
            "Special tokens to IDs: [('<pad>', 40000), ('<mask>', 40001), ('<s>', 1), ('</s>', 2), ('<unk>', 0)]\n",
            "IDs back to special tokens: ['<pad>', '<mask>', '<s>', '</s>', '<unk>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CustomSentencePieceTokenizer('/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/bpe.deen.model')"
      ],
      "metadata": {
        "id": "3WFG1iWM0xq7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load bpe lowercased data and save tokenized data"
      ],
      "metadata": {
        "id": "-58ExfKzz-QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def load_data(src_file, tgt_file):\n",
        "    with open(src_file, 'r', encoding='utf-8') as src_f, open(tgt_file, 'r', encoding='utf-8') as tgt_f:\n",
        "        src_lines = [line.strip() for line in src_f.readlines()]\n",
        "        tgt_lines = [line.strip() for line in tgt_f.readlines()]\n",
        "    assert len(src_lines) == len(tgt_lines), \"Source and target files should have the same number of lines.\"\n",
        "    return src_lines, tgt_lines\n",
        "\n",
        "def tokenize_and_save_data(src_lines, tgt_lines, tokenizer, src_path, tgt_path):\n",
        "    tokenized_src = [torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line)), dtype=torch.long) for line in src_lines]\n",
        "    tokenized_tgt = [torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line)), dtype=torch.long) for line in tgt_lines]\n",
        "    torch.save(tokenized_src, src_path)\n",
        "    torch.save(tokenized_tgt, tgt_path)\n",
        "    print(f\"Tokenized data saved to {src_path} and {tgt_path}\")\n"
      ],
      "metadata": {
        "id": "mVWKvpQUz81I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_file_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen.lc.src.bpe'\n",
        "tgt_file_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen.lc.tgt.bpe'\n",
        "model_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/bpe.deen.model'\n",
        "tokenized_src_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen_tokenized_src.pt'\n",
        "tokenized_tgt_path = '/content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen_tokenized_tgt.pt'"
      ],
      "metadata": {
        "id": "AQL1Hr350ns3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_lines, tgt_lines = load_data(src_file_path, tgt_file_path)\n",
        "tokenize_and_save_data(src_lines, tgt_lines, tokenizer, tokenized_src_path, tokenized_tgt_path)"
      ],
      "metadata": {
        "id": "aT-0iR8mtb-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b54591-ca6f-4622-f37c-eb0e1c46592e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized data saved to /content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen_tokenized_src.pt and /content/drive/MyDrive/bachelor_thesis/data/alignment-scripts/train/deen_tokenized_tgt.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartConfig, BartModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class BTBADecoderLayer(nn.Module):\n",
        "    def __init__(self, config, is_last_layer=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(config.d_model, config.decoder_attention_heads)\n",
        "        self.multihead_attn = nn.MultiheadAttention(config.d_model, config.decoder_attention_heads)\n",
        "        self.layer_norm1 = nn.LayerNorm(config.d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.d_model)\n",
        "        self.is_last_layer = is_last_layer\n",
        "        if not is_last_layer:\n",
        "            self.ffn = nn.Sequential(\n",
        "                nn.Linear(config.d_model, config.decoder_ffn_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(config.decoder_ffn_dim, config.d_model),\n",
        "            )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
        "        x = self.layer_norm1(x + self.dropout(self.self_attn(x, x, x, key_padding_mask=tgt_mask)[0]))\n",
        "        x = self.layer_norm2(x + self.dropout(self.multihead_attn(x, memory, memory, key_padding_mask=src_mask)[0]))\n",
        "        if not self.is_last_layer:\n",
        "            x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class BTBAModel(BartModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.decoder.layers = nn.ModuleList([\n",
        "            BTBADecoderLayer(config, is_last_layer=(i == config.decoder_layers - 1))\n",
        "            for i in range(config.decoder_layers)\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "vVYAdMJb1s1X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Masking for tje Dataset\n",
        "\n",
        "* Every word in a sentence should be masked only once across the entire training - track the masking state and reset it after each epoch.\n",
        "* Percentage-based: At least 10% of the words in each sentence must be masked, or one word if the sentence has less than ten words."
      ],
      "metadata": {
        "id": "ImlkdDaf19vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DynamicMaskingDataset(Dataset):\n",
        "    def __init__(self, tokenized_src, tokenized_tgt, tokenizer, mask_probability=0.1):\n",
        "        self.tokenized_src = tokenized_src\n",
        "        self.tokenized_tgt = tokenized_tgt\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_id = tokenizer.get_special_token_ids()['<mask>']\n",
        "        self.pad_id = tokenizer.get_special_token_ids()['<pad>']\n",
        "        self.mask_probability = mask_probability\n",
        "        self.mask_tracker = {i: set() for i in range(len(tokenized_tgt))}\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the number of items in the dataset\n",
        "        return len(self.tokenized_tgt)\n",
        "\n",
        "    def mask_input(self, inputs, idx):\n",
        "        num_tokens = len(inputs)\n",
        "        num_to_mask = max(int(num_tokens * self.mask_probability), 1)\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        candidate_mask = (inputs != self.pad_id) & (inputs != self.tokenizer.get_special_token_ids()['<s>']) & (inputs != self.tokenizer.get_special_token_ids()['</s>'])\n",
        "        candidate_indices = np.setdiff1d(np.where(candidate_mask.numpy())[0], list(self.mask_tracker[idx]))\n",
        "\n",
        "        if len(candidate_indices) == 0:\n",
        "            self.mask_tracker[idx] = set()\n",
        "            candidate_indices = np.where(candidate_mask.numpy())[0]\n",
        "\n",
        "        if len(candidate_indices) < num_to_mask:\n",
        "            num_to_mask = len(candidate_indices)\n",
        "\n",
        "        if num_to_mask > 0:\n",
        "            masked_indices = np.random.choice(candidate_indices, size=num_to_mask, replace=False)\n",
        "            self.mask_tracker[idx].update(masked_indices)\n",
        "            inputs[masked_indices] = self.mask_id\n",
        "        else:\n",
        "            labels.fill_(-100)\n",
        "\n",
        "        labels[~candidate_mask] = -100\n",
        "        return inputs, labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.tokenized_src[idx]\n",
        "        tgt = self.tokenized_tgt[idx]\n",
        "        src, src_labels = self.mask_input(src, idx)\n",
        "        tgt, tgt_labels = self.mask_input(tgt, idx)\n",
        "\n",
        "        return {\"input_ids\": src, \"labels\": tgt_labels}  # Ensure labels match the format expected by the model\n",
        "\n"
      ],
      "metadata": {
        "id": "l-JX0jEF1813"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_src = torch.load(tokenized_src_path)\n",
        "tokenized_tgt = torch.load(tokenized_tgt_path)"
      ],
      "metadata": {
        "id": "S480mEuc2s-b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DynamicMaskingDataset(tokenized_src, tokenized_tgt, tokenizer)"
      ],
      "metadata": {
        "id": "HJmML1Ly-nQk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartConfig\n",
        "\n",
        "# Update the model initialization\n",
        "config = BartConfig.from_pretrained('facebook/bart-large')\n",
        "config.decoder_ffn_dim = 3072\n",
        "model = BartForConditionalGeneration(config)\n",
        "\n",
        "# Now model.forward should be able to take 'labels' and compute loss if they are provided.\n"
      ],
      "metadata": {
        "id": "6c4_MsZp3EOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ed003cb64d0d4c12b8edb18584ad0e9b",
            "e46c8612c73b441d80e98fae13f8eeeb",
            "295f3956e5f346cfaf526319e0da3c6a",
            "da7805f3950c40d7805bcd0c9b6b34bf",
            "18ff8a5dc5d541348e75d85cfd3b3451",
            "10d679ba111347c392fb702b54936930",
            "ebb1b56af56246839a91f6acd95bea5c",
            "1849ee8975f34731bce298e833fa9016",
            "e88b101a7f1c4e688d1ea31244fc0c0d",
            "a0a6b3f3c01b4883af38e58070ace523",
            "e64d43ed390e4bdb844de23c0076a2f3"
          ]
        },
        "outputId": "44df8860-4f1b-469f-f657-c2de84d8be78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed003cb64d0d4c12b8edb18584ad0e9b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    labels = [item['labels'] for item in batch]\n",
        "\n",
        "    # Pad sequences to the longest sequence in the batch\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.get_special_token_ids()['<pad>'])\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "    return {'input_ids': input_ids_padded, 'labels': labels_padded}\n"
      ],
      "metadata": {
        "id": "W6OB11ii_EEc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if labels is not None:\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            labels = labels.view(-1)\n",
        "            loss = cross_entropy(logits, labels, ignore_index=-100)\n",
        "        else:\n",
        "            loss = None\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n"
      ],
      "metadata": {
        "id": "EwlIU6V9GIok"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np  # Ensure numpy is imported\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,  # Reduced batch size\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients to emulate a larger batch\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now you can create DataLoader and do other operations\n",
        "data_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,  # Adjust if necessary\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collate_fn,\n",
        ")\n"
      ],
      "metadata": {
        "id": "O40bT8gN245V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "83a51bd8-38c1-4d5a-9877-d5329202479d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-65d0a22b1b19>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m trainer = CustomTrainer(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         ):\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m                 )\n\u001b[0;32m-> 2796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     )\n\u001b[0;32m-> 1159\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jtm5ShTJpAyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Clearing the GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Checking for allocated memory and freeing up\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.ipc_collect()  # collect garbage\n"
      ],
      "metadata": {
        "id": "Nl-OYpzY54j4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_talp(model, tokenizer, data_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    alignments = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            outputs = model(input_ids)\n",
        "            generated_tokens = torch.argmax(outputs.logits, dim=-1)\n",
        "            alignments.extend(process_alignment_data(generated_tokens, tokenizer))\n",
        "    return alignments\n",
        "\n",
        "def process_alignment_data(generated_tokens, tokenizer):\n",
        "    # Convert tokens to alignments and return\n",
        "    return [tokenizer.convert_ids_to_tokens(g) for g in generated_tokens]\n"
      ],
      "metadata": {
        "id": "FUW0XXRq450F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_context_based_optimization(model, train_dataloader, optimizer, scheduler, device, num_iterations=50):\n",
        "    model.train()\n",
        "    for iteration in range(num_iterations):\n",
        "        for batch in train_dataloader:\n",
        "            input_ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
        "            outputs = model(input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "dN3eMzWn49E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def symmetrize_alignments(ltr_alignments):\n",
        "    symmetrical_alignments = {}\n",
        "    for src_idx, tgt_list in ltr_alignments.items():\n",
        "        for tgt_idx in tgt_list:\n",
        "            if tgt_idx not in symmetrical_alignments:\n",
        "                symmetrical_alignments[tgt_idx] = []\n",
        "            symmetrical_alignments[tgt_idx].append(src_idx)\n",
        "    # Ensure symmetry\n",
        "    for src_idx, tgt_list in ltr_alignments.items():\n",
        "        if src_idx not in symmetrical_alignments:\n",
        "            symmetrical_alignments[src_idx] = []\n",
        "        symmetrical_alignments[src_idx].extend([tgt for tgt in tgt_list if src_idx in symmetrical_alignments.get(tgt, [])])\n",
        "    return symmetrical_alignments\n"
      ],
      "metadata": {
        "id": "894-3JOg4_tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8tBYAa28g4Lu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}